{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "# from model.conv_ae import ConvolutionalEncoder, ConvolutionalDecoder, ConvolutionalAutoencoder\n",
    "# from model.sequence_encoder import SequenceEncoder\n",
    "# from model.sing import SINGModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Dataset related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare NSynth TFRecordDataset to pass to model\n",
    "def prepare_nsynth_dataset(dataset):\n",
    "    def process_parsed_features(point):\n",
    "        instrument = tf.one_hot(point['instrument'], 1006)\n",
    "        pitch = tf.one_hot(point['pitch'], 21)\n",
    "        velocity = tf.one_hot(point['velocity'], 5)\n",
    "        audio = point['audio']\n",
    "        inputs = tf.concat([instrument, pitch, velocity], axis = 0)\n",
    "        return {'inputs': inputs, 'outputs': audio}\n",
    "\n",
    "    def parse_nsynth(example_proto):\n",
    "        features = {\n",
    "            \"audio\": tf.io.FixedLenFeature((4 * 16000), tf.float32),\n",
    "            \"note\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "            \"note_str\": tf.io.FixedLenFeature((), dtype = tf.string),\n",
    "            \"instrument\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "            \"instrument_str\": tf.io.FixedLenFeature((), dtype = tf.string),\n",
    "            \"instrument_source\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "            \"instrument_source_str\": tf.io.FixedLenFeature((), dtype = tf.string),\n",
    "            \"instrument_family_str\": tf.io.FixedLenFeature((), dtype = tf.string),\n",
    "            \"sample_rate\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "            \"velocity\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "            \"pitch\": tf.io.FixedLenFeature((), dtype = tf.int64),\n",
    "        }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "        return process_parsed_features(parsed_features)\n",
    "\n",
    "    def tfr_dataset_eager(data, batch_size):\n",
    "        data = data.apply(tf.data.experimental.shuffle_and_repeat(10000))\n",
    "        data = data.apply(tf.data.experimental.map_and_batch(map_func = parse_nsynth, batch_size = batch_size))\n",
    "        data = data.prefetch(1)\n",
    "        return data\n",
    "\n",
    "    return tfr_dataset_eager(dataset, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_for_audio(raw_wav):\n",
    "    pass\n",
    "\n",
    "# Pass in tensor of (64000, ) to get the audio sample in return\n",
    "def generate_audio_sample(raw_wav):\n",
    "    np_wav = raw_wav.numpy()\n",
    "    print(np_wav)\n",
    "    Audio(np_wav, rate = 16000)\n",
    "    print('DOne!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create various models that we want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing = SING()\n",
    "conv_autoencoder = ConvolutionalAutoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0563812e-08 -1.1682036e-08  1.2283434e-08 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset('/data/NSynth/nsynth-train.tfrecord')\n",
    "dataset = prepare_nsynth_dataset(dataset)\n",
    "\n",
    "for i in dataset.take(1):\n",
    "    wav = i['outputs']\n",
    "    generate_audio_sample(wav)\n",
    "\n",
    "# print('Starting to train the Convolutional Autoencoder ...')\n",
    "# conv_ae_params = {}\n",
    "# conv_autoencoder.train(conv_ae_params)\n",
    "# print('Done!')\n",
    "\n",
    "# print('Starting to train the SING Model ...')\n",
    "# sing_params = {}\n",
    "# sing.train(sing_params)\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some audio from the train models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - Get some code in here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
